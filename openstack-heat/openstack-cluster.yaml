# Note - some of the defaults below are for an
# Alces Software OpenStack environment. 
# Please verify the configuration below before launching
# external_network: `public`
# clusterware image names: `clusterware-el6/7`
heat_template_version: 2013-05-23

description: >
  Launch an Alces Clusterware environment, complete with 1
  login node and 3 compute nodes.
  The stack includes all necessary separated networking
  to create a private research environment.
  All resources are destroyed on destruction of the stack

parameters:
  cluster_name:
    type: string
    label: Cluster name
    description: Cluster name
    default: hpc1

  cluster_user:
    type: string
    label: Cluster user username
    description: Cluster user username
    default: alces-cluster

  cluster_user_key:
    type: string
    label: Cluster user public SSH key
    description: Enter the cluster user public SSH key, e.g. `ssh-rsa 1234 user` 

  admin_key:
    type: string
    label: Cluster admin key
    description: Enter the name of an OpenStack nova keypair for administrator access

  cluster_type:
    type: string
    label: Cluster type
    description: Choose a cluster variant
    constraints:
    - allowed_values: [ clusterware-el6, clusterware-el7 ]

  compute_number:
    type: string
    label: Number of compute nodes
    description: Number of compute nodes to launch
    default: 3

  compute_flavour:
    type: string
    label: >
      Compute node instance type
    description: Compute node type 
    constraints:
    - allowed_values: [ m1.small, m1.medium, m1.large, m1.xlarge ]
      description: >
        Compute node instance type to launch

  gridware_depot:
    type: string
    label: Gridware Depot to install
    constraints:
    - allowed_values: [ bio, chem, benchmark ]

  aws_access_key:
    type: string
    label: AWS access key
    description: AWS access key to obtain Gridware Depot

  aws_secret:
    type: string
    label: AWS secret
    description: AWS secret to obtain Gridware Depot

resources:
  cluster_uuid:
    type: OS::Heat::RandomString
    properties:
      length: 24
      sequence: digits

  cluster_token:
    type: OS::Heat::RandomString
    properties:
      length: 20
      sequence: lettersdigits

  cluster_network:
    type: OS::Neutron::Net
    properties:
      name: { get_param: cluster_name }

  cluster_subnet:
    type: OS::Neutron::Subnet
    properties:
      network_id: { get_resource: cluster_network }
      cidr: 10.75.0.0/16
      gateway_ip: 10.75.0.254
      dns_nameservers: [ "8.8.8.8", "8.8.4.4" ]
      allocation_pools:
        - start: 10.75.0.2 
          end: 10.75.0.253

  router:
    type: OS::Neutron::Router
    properties:
      external_gateway_info:
        network: public

  router_interface:
    type: OS::Neutron::RouterInterface
    properties:
      router_id: { get_resource: router }
      subnet_id: { get_resource: cluster_subnet }

  cluster_sg:
    type: OS::Neutron::SecurityGroup
    properties:
      name: { get_param: cluster_name }
      description: Enable ping/SSH
      rules:
      - protocol: icmp
      - protocol: tcp
        port_range_min: 22
        port_range_max: 22

  master_config:
    type: OS::Heat::SoftwareConfig
    properties:
      group: script
      config:
        str_replace:
          template: |
            #!/bin/bash
            curl https://gist.githubusercontent.com/vlj91/93b04ab2a29b25dd91ed/raw/cc64c7364ea733ca78e3dcbc79d09eeb7968504a/login.sh > login.sh
            sh login.sh
            chown root:root /opt/clusterware/etc/config.yml
            chmod 0640 /opt/clusterware/etc/config.yml
            sed -e "s/%CWNAME%/%CLUSTER_NAME%/g" \
                -e "s/%CUNAME%/%CLUSTER_USER%/g" \
                -e "s,%CUKEY%,%CWUKEY%,g" \
                -e "s/%UUID%/%CWUUID%/g" \
                -e "s/%TOKEN%/%CWTOKEN%/g" \
                -e "s,%INSTALL%,%DEPOT%/g" \
                -e "s,%TKN,%AWSTOKEN%,g" \
                -e "s,%SCRT,%AWSSECRET%,g" \
                -i /opt/clusterware/etc/config.yml
          params:
            "%CLUSTER_NAME%": { get_param: cluster_name }
            "%CLUSTER_USER%": { get_param: cluster_user }
            "%CWUKEY%": { get_param: cluster_user_key }
            "%CWUUID%": { get_resource: cluster_uuid }
            "%CWTOKEN%": { get_resource: cluster_token }
            "%DEPOT%": { get_param: gridware_depot }

  node_config:
    type: OS::Heat::SoftwareConfig
    properties:
      group: script
      config:
        str_replace:
          template: |
            #!/bin/bash
            curl https://gist.githubusercontent.com/vlj91/7bfc57d11365ac2ef697/raw/c4999aad43b0028f77f461717ea0626586cd15e5/compute.sh > compute.sh
            sh compute.sh
            chown root:root /opt/clusterware/etc/config.yml
            chmod 0640 /opt/clusterware/etc/config.yml
            sed -e "s/%CWNAME%/%CLUSTER_NAME%/g" \
                -e "s/%CUNAME%/%CLUSTER_USER%/g" \
                -e "s,%CUKEY%,%CWUKEY%,g" \
                -e "s/%UUID%/%CWUUID%/g" \
                -e "s/%TOKEN%/%CWTOKEN%/g" \
                -i /opt/clusterware/etc/config.yml
          params:
            "%CLUSTER_NAME%": { get_param: cluster_name }
            "%CLUSTER_USER%": { get_param: cluster_user }
            "%CWUKEY": { get_param: cluster_user_key }
            "%CWUUID%": { get_resource: cluster_uuid }
            "%CWTOKEN%": { get_resource: cluster_token }

  login1:
    type: OS::Nova::Server
    properties:
      name: login1
      image: { get_param: cluster_type } 
      flavor: { get_param: compute_flavour }
      key_name: { get_param: admin_key }
      networks:
        - port: { get_resource: login1_port }
      user_data_format: RAW
      user_data: { get_resource: master_config }

  login1_port:
    type: OS::Neutron::Port
    properties:
      network_id: { get_resource: cluster_network }
      fixed_ips:
        - subnet_id: { get_resource: cluster_subnet }

  login1_access:
    type: OS::Neutron::FloatingIP
    properties:
      floating_network: public
      port_id: { get_resource: login1_port }

  group:
    type: OS::Heat::AutoScalingGroup
    depends_on: login1_access
    properties:
      cooldown: 60
      desired_capacity: { get_param: compute_number }
      max_size: 20
      min_size: 0
      resource:
        type: OS::Nova::Server
        properties:
          flavor: { get_param: compute_flavour }
          image: { get_param: cluster_type }
          user_data_format: RAW
          user_data: { get_resource: node_config }
          metadata: {"scale_group": { get_param: cluster_name }}
          networks: [{ network: { get_resource: cluster_network } }]

  scale_up_policy:
    type: OS::Heat::ScalingPolicy
    properties:
      adjustment_type: change_in_capacity
      auto_scaling_group_id: { get_resource: group }
      cooldown: 60
      scaling_adjustment: 1

  scale_down_policy:
    type: OS::Heat::ScalingPolicy
    properties:
      adjustment_type: change_in_capacity
      auto_scaling_group_id: { get_resource: group }
      cooldown: 60
      scaling_adjustment: -1

outputs:
  login1_public_ip:
    description: Floating IP address of login1 in public network
    value: { get_attr: [ login1_access, floating_ip_address ] }
  compute_up:
    description: >
      Increase the number of compute nodes using a HTTP POST to the
      scale_up URL.
    value: { get_attr: [ scale_up_policy, alarm_url ] }
  compute_down:
    description: >
      Decrease the number of compute nodes using a HTTP POST to the
      scale_down URL.
    value: { get_attr: [ scale_down_policy, alarm_url ] }
